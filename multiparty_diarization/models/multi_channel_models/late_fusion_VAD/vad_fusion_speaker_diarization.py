from pyannote.audio.pipelines.speaker_diarization import SpeakerDiarization
from pyannote.core import SlidingWindowFeature
import os

from pyannote.database import ProtocolFile
import textwrap
import warnings
from typing import Callable, Optional
import torchaudio
from itertools import permutations

import numpy as np
import torch
from pyannote.core import Annotation, SlidingWindowFeature

from pyannote.audio import Audio
from pyannote.audio.core.io import AudioFile

from pyannote.audio.utils.signal import binarize
from pyannote.audio.utils.reproducibility import fix_reproducibility

from multiparty_diarization.models.multi_channel_models.late_fusion_VAD.multi_channel_pyannote_vad import MultiChannelVAD

class VADFusionSpeakerDiarization(SpeakerDiarization):

    def __init__(self, *args, vad_weight: float = 0.5, **kwargs):

        super().__init__(*args, **kwargs)

        self.vad = MultiChannelVAD() # Wrapped VAD model
        self.vad_weight = vad_weight

        self.instantiate(self.default_parameters())
    
    def default_parameters(self):
        return {
            "segmentation": {
                        "threshold": 0.4442333667381752,
                        "min_duration_off": 0.0,
                    },
            "clustering": {
                        "method": "centroid",
                        "min_cluster_size": 15,
                        "threshold": 0.7153814381597874,
                    }
        }
    
    def __call__(self, array_file: AudioFile, lav_file: AudioFile, **kwargs):
        fix_reproducibility(getattr(self, "device", torch.device("cpu")))

        if not self.instantiated:
            # instantiate with default parameters when available
            try:
                default_parameters = self.default_parameters()
            except NotImplementedError:
                raise RuntimeError(
                    "A pipeline must be instantiated with `pipeline.instantiate(parameters)` before it can be applied."
                )

            try:
                self.instantiate(default_parameters)
            except ValueError:
                raise RuntimeError(
                    "A pipeline must be instantiated with `pipeline.instantiate(paramaters)` before it can be applied. "
                    "Tried to use parameters provided by `pipeline.default_parameters()` but those are not compatible. "
                )

            warnings.warn(
                f"The pipeline has been automatically instantiated with {default_parameters}."
            )

        array_file = Audio.validate_file(array_file)
        lav_file = Audio.validate_file(lav_file)

        if hasattr(self, "preprocessors"):
            file = ProtocolFile(file, lazy=self.preprocessors)

        return self.apply(array_file, lav_file, **kwargs)

    def apply(
        self,
        array_file: AudioFile,
        lav_file: AudioFile,
        min_speakers: int = None,
        max_speakers: int = None,
        return_embeddings: bool = False,
        hook: Optional[Callable] = None,
    ) -> Annotation:
        """Apply speaker diarization

        Parameters
        ----------
        array_file : AudioFile
            Processed array file.
        lav_file : AudioFile
            Processed lav file.
        num_speakers : int, optional
            Number of speakers, when known.
        min_speakers : int, optional
            Minimum number of speakers. Has no effect when `num_speakers` is provided.
        max_speakers : int, optional
            Maximum number of speakers. Has no effect when `num_speakers` is provided.
        return_embeddings : bool, optional
            Return representative speaker embeddings.
        hook : callable, optional
            Callback called after each major steps of the pipeline as follows:
                hook(step_name,      # human-readable name of current step
                     step_artefact,  # artifact generated by current step
                     file=file)      # file being processed
            Time-consuming steps call `hook` multiple times with the same `step_name`
            and additional `completed` and `total` keyword arguments usable to track
            progress of current step.

        Returns
        -------
        diarization : Annotation
            Speaker diarization
        embeddings : np.array, optional
            Representative speaker embeddings such that `embeddings[i]` is the
            speaker embedding for i-th speaker in diarization.labels().
            Only returned when `return_embeddings` is True.
        """

        # setup hook (e.g. for debugging purposes)
        hook = self.setup_hook(array_file, hook=hook)

        num_speakers = lav_file['waveform'].shape[0]

        num_speakers, min_speakers, max_speakers = self.set_num_speakers(
            num_speakers=num_speakers,
            min_speakers=num_speakers,
            max_speakers=num_speakers,
        )

        segmentations = self.get_segmentations(array_file, hook=hook)
        hook("segmentation", segmentations)
        #   shape: (num_chunks, num_frames, local_num_speakers)

        # binarize segmentation
        if self._segmentation.model.specifications.powerset:
            binarized_segmentations = segmentations
        else:
            binarized_segmentations: SlidingWindowFeature = binarize(
                segmentations,
                onset=self.segmentation.threshold,
                initial_state=False,
            )

        # estimate frame-level number of instantaneous speakers
        count = self.speaker_count(
            binarized_segmentations,
            frames=self._frames,
            warm_up=(0.0, 0.0),
        )
        hook("speaker_counting", count)
        #   shape: (num_frames, 1)
        #   dtype: int

        # exit early when no speaker is ever active
        if np.nanmax(count.data) == 0.0:
            diarization = Annotation(uri=array_file["uri"])
            if return_embeddings:
                return diarization, np.zeros((0, self._embedding.dimension))

            return diarization

        if self.klustering == "OracleClustering" and not return_embeddings:
            embeddings = None
        else:
            embeddings = self.get_embeddings(
                array_file,
                binarized_segmentations,
                exclude_overlap=self.embedding_exclude_overlap,
                hook=hook,
            )
            hook("embeddings", embeddings)
            #   shape: (num_chunks, local_num_speakers, dimension)

        hard_clusters, _, centroids = self.clustering(
            embeddings=embeddings,
            segmentations=binarized_segmentations,
            num_clusters=num_speakers,
            min_clusters=num_speakers,
            max_clusters=num_speakers,
            file=array_file,  # <== for oracle clustering
            frames=self._frames,  # <== for oracle clustering
        )
        # hard_clusters: (num_chunks, num_speakers)
        # centroids: (num_speakers, dimension)

        # number of detected clusters is the number of different speakers
        num_different_speakers = np.max(hard_clusters) + 1

        # detected number of speakers can still be out of bounds
        # (specifically, lower than `min_speakers`), since there could be too few embeddings
        # to make enough clusters with a given minimum cluster size.
        if num_different_speakers < min_speakers or num_different_speakers > max_speakers:
            warnings.warn(textwrap.dedent(
                f"""
                The detected number of speakers ({num_different_speakers}) is outside
                the given bounds [{min_speakers}, {max_speakers}]. This can happen if the
                given audio file is too short to contain {min_speakers} or more speakers.
                Try to lower the desired minimal number of speakers.
                """
            ))

        # during counting, we could possibly overcount the number of instantaneous
        # speakers due to segmentation errors, so we cap the maximum instantaneous number
        # of speakers by the `max_speakers` value
        count.data = np.minimum(count.data, max_speakers).astype(np.int8)

        # reconstruct discrete diarization from raw hard clusters

        # keep track of inactive speakers
        inactive_speakers = np.sum(binarized_segmentations.data, axis=1) == 0
        #   shape: (num_chunks, num_speakers)

        hard_clusters[inactive_speakers] = -2
        clustered_segmentations, count = self.reconstruct(
            segmentations,
            hard_clusters,
            count,
        )

        # ~VAD~ #
        # Call VAD model
        lav_vad = self.vad(lav_file)

        array_vad = clustered_segmentations.data
        array_vad, lav_vad = self.align_vad_predictions(array_vad, lav_vad, num_speakers=num_speakers)
        # ~VAD~ #

        # Take weighted average with VAD
        clustered_segmentations.data = (1 - self.vad_weight) * array_vad + self.vad_weight * lav_vad

        discrete_diarization = self.to_diarization(clustered_segmentations, count)
        hook("discrete_diarization", discrete_diarization)

        # convert to continuous diarization
        diarization = self.to_annotation(
            discrete_diarization,
            min_duration_on=0.0,
            min_duration_off=self.segmentation.min_duration_off,
        )
        diarization.uri = array_file["uri"]

        # at this point, `diarization` speaker labels are integers
        # from 0 to `num_speakers - 1`, aligned with `centroids` rows.

        if "annotation" in array_file and array_file["annotation"]:
            # when reference is available, use it to map hypothesized speakers
            # to reference speakers (this makes later error analysis easier
            # but does not modify the actual output of the diarization pipeline)
            _, mapping = self.optimal_mapping(
                array_file["annotation"], diarization, return_mapping=True
            )

            # in case there are more speakers in the hypothesis than in
            # the reference, those extra speakers are missing from `mapping`.
            # we add them back here
            mapping = {key: mapping.get(key, key) for key in diarization.labels()}

        else:
            # when reference is not available, rename hypothesized speakers
            # to human-readable SPEAKER_00, SPEAKER_01, ...
            mapping = {
                label: expected_label
                for label, expected_label in zip(diarization.labels(), self.classes())
            }

        diarization = diarization.rename_labels(mapping=mapping)

        # at this point, `diarization` speaker labels are strings (or mix of
        # strings and integers when reference is available and some hypothesis
        # speakers are not present in the reference)

        if not return_embeddings:
            return diarization

        # this can happen when we use OracleClustering
        if centroids is None:
            return diarization, None

        # The number of centroids may be smaller than the number of speakers
        # in the annotation. This can happen if the number of active speakers
        # obtained from `speaker_count` for some frames is larger than the number
        # of clusters obtained from `clustering`. In this case, we append zero embeddings
        # for extra speakers
        if len(diarization.labels()) > centroids.shape[0]:
            centroids = np.pad(centroids, ((0, len(diarization.labels()) - centroids.shape[0]), (0, 0)))

        # re-order centroids so that they match
        # the order given by diarization.labels()
        inverse_mapping = {label: index for index, label in mapping.items()}
        centroids = centroids[
            [inverse_mapping[label] for label in diarization.labels()]
        ]

        return diarization, centroids
    
    def align_vad_predictions(self, array_vad: np.ndarray, lav_vad: np.ndarray, num_speakers: int):
        
        assert lav_vad.shape[-1] == num_speakers, 'should be one lav channel per speaker'

        num_array_pred_speakers = array_vad.shape[-1]

        if num_array_pred_speakers > num_speakers:
            chans_to_add = num_array_pred_speakers - num_speakers
            lav_vad = np.concatenate((lav_vad, np.zeros((*lav_vad.shape[:-1], chans_to_add))), axis=-1)

        elif num_array_pred_speakers < num_speakers:
            chans_to_add = num_speakers - num_array_pred_speakers
            array_vad = np.concatenate((array_vad, np.zeros((*array_vad.shape[:-1], chans_to_add))), axis=-1)
        
        # Find best channel aligment between VAD and array predictions
        lav_vad_flat = lav_vad.reshape(-1, num_speakers)
        array_vad_flat = array_vad.reshape(-1, num_speakers)
        array_vad_flat = np.nan_to_num(array_vad_flat, nan=0.0)

        # Normalize per channel
        lav_vad_flat = lav_vad_flat/np.linalg.norm(lav_vad_flat, axis=0, keepdims=True)
        array_vad_flat = array_vad_flat/np.linalg.norm(array_vad_flat, axis=0, keepdims=True)

        all_permutations = list(permutations(range(lav_vad.shape[-1])))
        permutation_dists = []
        for perm in all_permutations: # Test all possible channel permulations
            perm = np.array(perm)
            permutation_dists.append( 
                    np.linalg.norm(array_vad_flat - lav_vad_flat[:, perm]) 
                )

        # Find best permutation
        best_perm = np.array(
            all_permutations[np.argmin(permutation_dists)]
        )

        if num_array_pred_speakers > num_speakers:
            lav_vad = lav_vad[:,:,:-chans_to_add] # Drop extra channels
            array_chans_to_keep = np.array([i for i in range(num_array_pred_speakers) if i in best_perm[:-chans_to_add]])
            array_vad = array_vad[:, :, array_chans_to_keep]
            best_perm = best_perm[array_chans_to_keep]
 
        lav_vad = lav_vad[:, :, best_perm]

        return array_vad, lav_vad
    
    def reconstruct(
        self,
        segmentations: SlidingWindowFeature,
        hard_clusters: np.ndarray,
        count: SlidingWindowFeature,
    ) -> SlidingWindowFeature:
        """Build final discrete diarization out of clustered segmentation

        Parameters
        ----------
        segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature
            Raw speaker segmentation.
        hard_clusters : (num_chunks, num_speakers) array
            Output of clustering step.
        count : (total_num_frames, 1) SlidingWindowFeature
            Instantaneous number of active speakers.

        Returns
        -------
        discrete_diarization : SlidingWindowFeature
            Discrete (0s and 1s) diarization.
        """

        num_chunks, num_frames, local_num_speakers = segmentations.data.shape

        num_clusters = np.max(hard_clusters) + 1
        clustered_segmentations = np.NAN * np.zeros(
            (num_chunks, num_frames, num_clusters)
        )

        for c, (cluster, (chunk, segmentation)) in enumerate(
            zip(hard_clusters, segmentations)
        ):
            # cluster is (local_num_speakers, )-shaped
            # segmentation is (num_frames, local_num_speakers)-shaped
            for k in np.unique(cluster):
                if k == -2:
                    continue

                # TODO: can we do better than this max here?
                clustered_segmentations[c, :, k] = np.max(
                    segmentation[:, cluster == k], axis=1
                )

        clustered_segmentations = SlidingWindowFeature(
            clustered_segmentations, segmentations.sliding_window
        )

        return clustered_segmentations, count
    
if __name__ == "__main__":

    import torch
    import torchaudio

    HUGGING_FACE_TOKEN = os.environ.get('HUGGING_FACE_TOKEN')

    model = VADFusionSpeakerDiarization()

    waveform, _ = torchaudio.load('/home/nmehlman/disney/multiparty_diarization/misc/test_audio.wav')

    array_file = {"waveform": waveform, "sample_rate": 16000}

    multi_channel_waveform = torch.concat([waveform, waveform, waveform, waveform], 0)
    multi_channel_waveform += torch.randn_like(multi_channel_waveform)
    lav_file = {"waveform": multi_channel_waveform, "sample_rate": 16000}

    diarization_audio = model(
        array_file,
        lav_file
    )